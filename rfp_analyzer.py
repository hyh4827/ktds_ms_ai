import json
import os
from datetime import datetime
from typing import Dict, List, Any
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.models import VectorizedQuery
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes.models import (
    SearchIndex,
    SearchField,
    SearchFieldDataType,
    SimpleField,
    SearchableField,
    VectorSearch,
    HnswAlgorithmConfiguration,
    VectorSearchProfile,
    SemanticConfiguration,
    SemanticPrioritizedFields,
    SemanticField,
    SemanticSearch
)
import openai
from openai import AzureOpenAI
import PyPDF2
import docx
from io import BytesIO
import re
import streamlit as st

class RFPAnalyzer:
    def __init__(self):
        self.search_client = None
        self.openai_client = None
        self.index_name = "rfp-documents"
        
    def initialize_services(self, search_endpoint, search_key, openai_endpoint, openai_key, openai_api_version):
        """Azure ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # Azure AI Search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            credential = AzureKeyCredential(search_key)
            self.search_client = SearchClient(
                endpoint=search_endpoint,
                index_name=self.index_name,
                credential=credential
            )
            
            # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.openai_client = AzureOpenAI(
                azure_endpoint=openai_endpoint,
                api_key=openai_key,
                api_version=openai_api_version
            )
            
            return True
        except Exception as e:
            st.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            return False
    
    def create_search_index(self, search_endpoint, search_key):
        """Azure AI Search ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            credential = AzureKeyCredential(search_key)
            index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)
            
            # ì¸ë±ìŠ¤ ì •ì˜
            fields = [
                SimpleField(name="id", type=SearchFieldDataType.String, key=True),
                SearchableField(name="title", type=SearchFieldDataType.String),
                SearchableField(name="content", type=SearchFieldDataType.String),
                SearchableField(name="requirements", type=SearchFieldDataType.String),
                SearchableField(name="project_type", type=SearchFieldDataType.String),
                SearchableField(name="budget_range", type=SearchFieldDataType.String),
                SimpleField(name="submission_deadline", type=SearchFieldDataType.String),
                SearchableField(name="evaluation_criteria", type=SearchFieldDataType.String),
                SimpleField(name="created_date", type=SearchFieldDataType.DateTimeOffset),
                SearchField(name="content_vector", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                           searchable=True, vector_search_dimensions=1536, vector_search_profile_name="myHnswProfile")
            ]
            
            # Vector search êµ¬ì„±
            vector_search = VectorSearch(
                algorithms=[
                    HnswAlgorithmConfiguration(name="myHnsw")
                ],
                profiles=[
                    VectorSearchProfile(
                        name="myHnswProfile",
                        algorithm_configuration_name="myHnsw"
                    )
                ]
            )
            
            # Semantic search êµ¬ì„±
            semantic_config = SemanticConfiguration(
                name="my-semantic-config",
                prioritized_fields=SemanticPrioritizedFields(
                    title_field=SemanticField(field_name="title"),
                    content_fields=[SemanticField(field_name="content")]
                )
            )
            
            semantic_search = SemanticSearch(configurations=[semantic_config])
            
            # ì¸ë±ìŠ¤ ìƒì„±
            index = SearchIndex(
                name=self.index_name,
                fields=fields,
                vector_search=vector_search,
                semantic_search=semantic_search
            )
            
            index_client.create_or_update_index(index)
            return True
            
        except Exception as e:
            st.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return False
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            max_chunk_size = 4000  # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ 4000ìë¡œ ì„¤ì • (ì•½ 6000 í† í°)
            
            if len(text) <= max_chunk_size:
                # í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                response = self.openai_client.embeddings.create(
                    input=text,
                    model="text-embedding-ada-002"
                )
                return response.data[0].embedding
            else:
                # í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                st.warning(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ê¸¸ì´: {len(text)} ë¬¸ì)")
                
                # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = self._split_text_into_chunks(text, max_chunk_size)
                embeddings = []
                
                progress_bar = st.progress(0)
                for i, chunk in enumerate(chunks):
                    st.write(f"   ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
                    response = self.openai_client.embeddings.create(
                        input=chunk,
                        model="text-embedding-ada-002"
                    )
                    embeddings.append(response.data[0].embedding)
                    progress_bar.progress((i + 1) / len(chunks))
                
                # ëª¨ë“  ì²­í¬ì˜ ì„ë² ë”©ì„ í‰ê· í™”í•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“¦
                if embeddings:
                    avg_embedding = np.mean(embeddings, axis=0).tolist()
                    return avg_embedding
                else:
                    return []
                    
        except Exception as e:
            st.error(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _split_text_into_chunks(self, text: str, max_chunk_size: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
        chunks = []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• ì„ ì‹œë„
        sentences = text.split('. ')
        current_chunk = ""
        
        for sentence in sentences:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ í™•ì¸
            test_chunk = current_chunk + sentence + ". " if current_chunk else sentence + ". "
            
            if len(test_chunk) <= max_chunk_size:
                current_chunk = test_chunk
            else:
                # í˜„ì¬ ì²­í¬ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
                else:
                    # ë¬¸ì¥ ìì²´ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ê°•ì œë¡œ ìë¦„
                    if len(sentence) > max_chunk_size:
                        # ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¦„
                        words = sentence.split()
                        temp_chunk = ""
                        
                        for word in words:
                            if len(temp_chunk + " " + word) <= max_chunk_size:
                                temp_chunk += " " + word if temp_chunk else word
                            else:
                                if temp_chunk:
                                    chunks.append(temp_chunk.strip())
                                temp_chunk = word
                        
                        if temp_chunk:
                            current_chunk = temp_chunk + ". "
                    else:
                        current_chunk = sentence + ". "
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # ê° ì²­í¬ì˜ í¬ê¸°ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ì¶”ê°€ ë¶„í• 
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= max_chunk_size:
                final_chunks.append(chunk)
            else:
                # ì²­í¬ê°€ ì—¬ì „íˆ í¬ë©´ ì¶”ê°€ë¡œ ë¶„í• 
                words = chunk.split()
                temp_chunk = ""
                
                for word in words:
                    if len(temp_chunk + " " + word) <= max_chunk_size:
                        temp_chunk += " " + word if temp_chunk else word
                    else:
                        if temp_chunk:
                            final_chunks.append(temp_chunk.strip())
                        temp_chunk = word
                
                if temp_chunk:
                    final_chunks.append(temp_chunk.strip())
        
        return final_chunks
    
    def extract_text_from_file(self, file_path):
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # íŒŒì¼ ê²½ë¡œ ì •ê·œí™” ë° ì¡´ì¬ í™•ì¸
            file_path = os.path.abspath(file_path)
            st.info(f"ğŸ” íŒŒì¼ ê²½ë¡œ: {file_path}")
            
            if not os.path.exists(file_path):
                st.error(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                return None, None
            
            if not os.path.isfile(file_path):
                st.error(f"âŒ ê²½ë¡œê°€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {file_path}")
                return None, None
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(file_path)
            st.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size} bytes")
            
            if file_size == 0:
                st.error("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None, None
            
            file_extension = os.path.splitext(file_path)[1].lower()
            st.info(f"ğŸ“„ íŒŒì¼ í™•ì¥ì: {file_extension}")
            
            if file_extension == ".pdf":
                st.info("ğŸ“– PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    total_pages = len(pdf_reader.pages)
                    st.info(f"ğŸ“„ PDF í˜ì´ì§€ ìˆ˜: {total_pages}")
                    
                    # PDF ì œëª© ì¶”ì¶œ ì‹œë„
                    pdf_title = None
                    try:
                        if pdf_reader.metadata and pdf_reader.metadata.title:
                            pdf_title = pdf_reader.metadata.title.strip()
                            st.info(f"ğŸ“‹ PDF ì œëª©: {pdf_title}")
                    except:
                        pass
                    
                    # ì œëª©ì´ ì—†ìœ¼ë©´ íŒŒì¼ëª… ì‚¬ìš©
                    if not pdf_title:
                        pdf_title = os.path.splitext(os.path.basename(file_path))[0]
                        st.info(f"ğŸ“‹ íŒŒì¼ëª…ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©: {pdf_title}")
                    
                    # í˜ì´ì§€ ìˆ˜ ì œí•œ (200í˜ì´ì§€)
                    max_pages = 200
                    if total_pages > max_pages:
                        st.warning(f"âš ï¸ PDF í˜ì´ì§€ ìˆ˜ê°€ {max_pages}í˜ì´ì§€ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì²˜ìŒ {max_pages}í˜ì´ì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                        pages_to_process = max_pages
                    else:
                        pages_to_process = total_pages
                    
                    text = ""
                    progress_bar = st.progress(0)
                    for i in range(pages_to_process):
                        try:
                            page = pdf_reader.pages[i]
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                            # st.write(f"   í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì™„ë£Œ")
                            progress_bar.progress((i + 1) / pages_to_process)
                        except Exception as page_error:
                            st.warning(f"   í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {str(page_error)}")
                            continue
                    
                    if not text.strip():
                        st.error("âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return None, None
                    
                    st.success(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ë¬¸ì (ì²˜ë¦¬ëœ í˜ì´ì§€: {pages_to_process}/{total_pages})")
                    return text, pdf_title
            
            elif file_extension == ".docx":
                st.info("ğŸ“ DOCX íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
                doc = docx.Document(file_path)
                st.info(f"ğŸ“„ DOCX ë‹¨ë½ ìˆ˜: {len(doc.paragraphs)}")
                
                # DOCX ì œëª© ì¶”ì¶œ ì‹œë„ (ì²« ë²ˆì§¸ ë‹¨ë½ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©)
                docx_title = None
                if doc.paragraphs:
                    first_paragraph = doc.paragraphs[0].text.strip()
                    if first_paragraph and len(first_paragraph) < 100:  # ë„ˆë¬´ ê¸¸ë©´ ì œëª©ì´ ì•„ë‹ ê°€ëŠ¥ì„±
                        docx_title = first_paragraph
                        st.info(f"ğŸ“‹ DOCX ì œëª©: {docx_title}")
                
                # ì œëª©ì´ ì—†ìœ¼ë©´ íŒŒì¼ëª… ì‚¬ìš©
                if not docx_title:
                    docx_title = os.path.splitext(os.path.basename(file_path))[0]
                    st.info(f"ğŸ“‹ íŒŒì¼ëª…ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©: {docx_title}")
                
                text = ""
                for i, paragraph in enumerate(doc.paragraphs):
                    if paragraph.text.strip():
                        text += paragraph.text + "\n"
                
                if not text.strip():
                    st.error("âŒ DOCXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None, None
                
                st.success(f"âœ… DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ë¬¸ì")
                return text, docx_title
            
            elif file_extension == ".txt":
                st.info("ğŸ“„ TXT íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
                with open(file_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                    if not content.strip():
                        st.error("âŒ TXT íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                        return None, None
                    
                    # TXT ì œëª© ì¶”ì¶œ ì‹œë„ (ì²« ë²ˆì§¸ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©)
                    txt_title = None
                    first_line = content.split('\n')[0].strip()
                    if first_line and len(first_line) < 100:  # ë„ˆë¬´ ê¸¸ë©´ ì œëª©ì´ ì•„ë‹ ê°€ëŠ¥ì„±
                        txt_title = first_line
                        st.info(f"ğŸ“‹ TXT ì œëª©: {txt_title}")
                    
                    # ì œëª©ì´ ì—†ìœ¼ë©´ íŒŒì¼ëª… ì‚¬ìš©
                    if not txt_title:
                        txt_title = os.path.splitext(os.path.basename(file_path))[0]
                        st.info(f"ğŸ“‹ íŒŒì¼ëª…ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©: {txt_title}")
                    
                    st.success(f"âœ… TXT í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(content)} ë¬¸ì")
                    return content, txt_title
            
            else:
                st.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
                st.info("   ì§€ì› í˜•ì‹: .pdf, .docx, .txt")
                return None, None
                
        except FileNotFoundError as e:
            st.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            return None, None
        except PermissionError as e:
            st.error(f"âŒ íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            return None, None
        except UnicodeDecodeError as e:
            st.error(f"âŒ íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜: {str(e)}")
            st.info("   ë‹¤ë¥¸ ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
            return None, None
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            st.info(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None, None
    
    def analyze_rfp_with_gpt(self, rfp_content: str) -> Dict[str, Any]:
        """GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ RFP ë‚´ìš© ë¶„ì„"""
        prompt = f"""
        RFP ë‚´ìš©ì„ 11ê°œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ì„í•˜ì„¸ìš”:
        
        **ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”):**
        1. RFP ë¬¸ì„œ ì „ì²´ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ ê¼¼ê¼¼íˆ ê²€í† í•˜ì—¬ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ì°¾ì•„ì„œ ì¶”ì¶œí•˜ì„¸ìš”
        2. ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸ íŒ¨í„´: ECR-XXX-XXX-XX, REQ-XXX-XXX, RFP-XXX-XXX, REQ-XX-XX-XX, REQ-001, REQ-002 ë“±
        3. ê° ìš”êµ¬ì‚¬í•­ë³„ë¡œ ë¶„ë¥˜, ëª…ì¹­, ì„¸ë¶€ë‚´ìš©, ì‚°ì¶œì •ë³´ í¬í•¨
        4. ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œ (ê³ ìœ ë²ˆí˜¸ ì—†ëŠ” ê²½ìš° REQ-GEN-001, REQ-GEN-002 í˜•íƒœë¡œ ë¶€ì—¬)
        5. ìš”êµ¬ì‚¬í•­ì„ ì°¾ì„ ë•ŒëŠ” "ìš”êµ¬ì‚¬í•­", "ê¸°ëŠ¥ìš”êµ¬", "ë¹„ê¸°ëŠ¥ìš”êµ¬", "ì„±ëŠ¥ìš”êµ¬", "ë³´ì•ˆìš”êµ¬", "ì›¹ì ‘ê·¼ì„±", "ê°œì„ ", "êµ¬ì¶•", "ê°œë°œ" ë“±ì˜ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì„¸ìš”
        6. ë¬¸ì„œì—ì„œ ìš”êµ¬ì‚¬í•­ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë¬¸ì¥, ë¬¸ë‹¨ì„ ì°¾ì•„ì„œ ê°ê°ì„ ë³„ë„ì˜ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
        7. ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡ ë°°ì—´ì—ëŠ” ì‹¤ì œ ë¬¸ì„œì—ì„œ ì°¾ì€ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš” (ìµœì†Œ 50ê°œ ì´ìƒ)
        8. ì˜ˆì‹œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , ì‹¤ì œ RFP ë¬¸ì„œì—ì„œ ì°¾ì€ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”
        
        **ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ë°©ë²•:**
        - ë¬¸ì„œì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ê²€í† í•˜ì„¸ìš”
        - ìš”êµ¬ì‚¬í•­ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë¬¸ì¥ì„ ì°¾ìœ¼ì„¸ìš”
        - ê° ìš”êµ¬ì‚¬í•­ì„ ë³„ë„ì˜ í•­ëª©ìœ¼ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”
        - ê³ ìœ ë²ˆí˜¸ê°€ ì—†ëŠ” ìš”êµ¬ì‚¬í•­ë„ REQ-GEN-001, REQ-GEN-002 í˜•íƒœë¡œ ë¶€ì—¬í•˜ì„¸ìš”
        - ìµœì†Œ 50ê°œ ì´ìƒì˜ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•˜ì„¸ìš”
        
        **ì¤‘ìš”: ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡ì—ëŠ” ì‹¤ì œ RFP ë¬¸ì„œì—ì„œ ì°¾ì€ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”. ì˜ˆì‹œë¥¼ ë³µì‚¬í•˜ì§€ ë§ˆì„¸ìš”.**
        
        **ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡ í˜•ì‹:**
        ê° ìš”êµ¬ì‚¬í•­ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
        {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "ì‹¤ì œ_ê³ ìœ ë²ˆí˜¸", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ì‹¤ì œ_ë¶„ë¥˜", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ì‹¤ì œ_ëª…ì¹­", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ_ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ_ì‚°ì¶œë¬¼"]}}
        
        **ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡ ì‘ì„± ê·œì¹™:**
        - ì‹¤ì œ RFP ë¬¸ì„œì—ì„œ ì°¾ì€ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
        - ì˜ˆì‹œë¥¼ ë³µì‚¬í•˜ì§€ ë§ˆì„¸ìš”
        - ìµœì†Œ 50ê°œ ì´ìƒì˜ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•˜ì„¸ìš”
        - ê° ìš”êµ¬ì‚¬í•­ì€ ë³„ë„ì˜ ê°ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”
        
        **ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡ ì˜ˆì‹œ (ì°¸ê³ ìš©):**
        [
            {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "REQ-001", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ê¸°ëŠ¥ìš”êµ¬", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ì›¹ì ‘ê·¼ì„± ê°œì„ ", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}},
            {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "ECR-002", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ë¹„ê¸°ëŠ¥ìš”êµ¬", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ì„±ëŠ¥ ìµœì í™”", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}},
            {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "DAR-003", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ë³´ì•ˆìš”êµ¬", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ë³´ì•ˆ ìš”êµ¬ì‚¬í•­", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}},
            {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "FUN-004", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "í˜¸í™˜ì„±í‘œì¤€", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "í˜¸í™˜ì„± ìš”êµ¬ì‚¬í•­", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}}
        ]
        
        **ìœ„ ì˜ˆì‹œëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ RFP ë¬¸ì„œì—ì„œ ì°¾ì€ ëª¨ë“  ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.**
        
        **ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‚¬í•­:**
        1. ì‹¤ì œ RFP ë¬¸ì„œì—ì„œ ì°¾ì€ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
        2. ì˜ˆì‹œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ˆì„¸ìš”
        3. ìµœì†Œ 50ê°œ ì´ìƒì˜ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•˜ì„¸ìš”
        4. ê° ìš”êµ¬ì‚¬í•­ì€ ë³„ë„ì˜ ê°ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”
        5. ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸ëŠ” ì‹¤ì œ ë¬¸ì„œì—ì„œ ì°¾ì€ ê²ƒì„ ì‚¬ìš©í•˜ì„¸ìš”
        6. ìš”êµ¬ì‚¬í•­ ë¶„ë¥˜ëŠ” ì‹¤ì œ ë‚´ìš©ì— ë§ê²Œ ë¶„ë¥˜í•˜ì„¸ìš”
        7. ìš”êµ¬ì‚¬í•­ ëª…ì¹­ì€ ì‹¤ì œ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
        8. ìš”êµ¬ì‚¬í•­ ì„¸ë¶€ë‚´ìš©ì€ ì‹¤ì œ ë‚´ìš©ì„ ìƒì„¸íˆ ì‘ì„±í•˜ì„¸ìš”
        9. ì‚°ì¶œì •ë³´ëŠ” ì‹¤ì œ ì‚°ì¶œë¬¼ì„ ëª…ì‹œí•˜ì„¸ìš”
        
        RFP ë‚´ìš©:
        {rfp_content}

        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "1_í•µì‹¬ê°œìš”": {{
                "ë°°ê²½ëª©ì ": "í”„ë¡œì íŠ¸ ë°°ê²½ ë° ëª©ì ",
                "ë²”ìœ„": "í”„ë¡œì íŠ¸ ë²”ìœ„ (í¬í•¨/ì œì™¸ ì‚¬í•­)",
                "ê¸°ëŒ€ì„±ê³¼": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ ë° íš¨ê³¼ ì§€í‘œ",
                "ìš©ì–´ì •ì˜": "ì£¼ìš” ìš©ì–´ ë° ì•½ì–´ ì •ì˜",
                "ì´í•´ê´€ê³„ì": "ë°œì£¼ë¶€ì„œ ë° ì´í•´ê´€ê³„ì"
            }},
            "2_ì¼ì •ë§ˆì¼ìŠ¤í†¤": {{
                "ì‚¬ì—…ê¸°ê°„": "ì°©ìˆ˜ì¼ë¶€í„° ì¢…ë£Œì¼ê¹Œì§€",
                "ì£¼ìš”ë§ˆì¼ìŠ¤í†¤": "ì°©ìˆ˜/ì¤‘ê°„ì ê²€/ì‹œë²”/ê²€ìˆ˜ ì¼ì •",
                "ì œì¶œë¬¼ì¼ì •": "ìš”êµ¬ì„œ/ì„¤ê³„/ê²°ê³¼ë³´ê³  ë“± ì œì¶œë¬¼ ì¼ì •",
                "ì§ˆì˜ì‘ë‹µë§ˆê°": "Q&A ë° ì œì•ˆì„œ ì ‘ìˆ˜ ë§ˆê°ì¼"
            }},
            "3_ì˜ˆì‚°ê°€ê²©": {{
                "ì¶”ì •ì˜ˆì‚°": "ì˜ˆì‚° ë²”ìœ„ ë° ìƒí•œê°€",
                "ë¶€ê°€ì„¸í¬í•¨": "ë¶€ê°€ì„¸ í¬í•¨ ì—¬ë¶€",
                "ê°€ê²©êµ¬ì„±": "ë¼ì´ì„ ìŠ¤/êµ¬ì¶•/ìš´ì˜/êµìœ¡/ì˜µì…˜ ë¹„ìš©",
                "ì§€ë¶ˆì¡°ê±´": "ì„ ê¸‰/ì¤‘ë„/ì¤€ê³µ/ê²€ìˆ˜ ì—°ë™ ì§€ë¶ˆ ì¡°ê±´",
                "ì›ê°€ì‚°ì¶œê·¼ê±°": "ì¸ë ¥ë‹¨ê°€, ìˆ˜ëŸ‰, ì‚°ì‹ ë“±"
            }},
            "4_í‰ê°€ì„ ì •ê¸°ì¤€": {{
                "ì •ëŸ‰ì •ì„±ë°°ì ": "ê¸°ìˆ /ê°€ê²© ë¹„ìœ¨ ë° ë°°ì í‘œ",
                "ê°€ì ê°ì ìš”ê±´": "ë ˆí¼ëŸ°ìŠ¤, ì¸ì¦, í˜„ì¥ì‹¤ì‚¬ ë“±",
                "íƒˆë½í•„ìˆ˜ìš”ê±´": "í•„ìˆ˜ ì„œë¥˜ ë° ìê²© ë¯¸ì¶©ì¡± ì‹œ íƒˆë½ ì¡°ê±´"
            }},
            "5_ìš”êµ¬ì‚¬í•­": {{
                "ìš”êµ¬ì‚¬í•­_ìƒì„¸ëª©ë¡": [
                    {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "REQ-001", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ê¸°ëŠ¥ìš”êµ¬", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ì›¹ì ‘ê·¼ì„± ê°œì„ ", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}},
                    {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "ECR-002", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ë¹„ê¸°ëŠ¥ìš”êµ¬", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ì„±ëŠ¥ ìµœì í™”", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}},
                    {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "DAR-003", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "ë³´ì•ˆìš”êµ¬", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "ë³´ì•ˆ ìš”êµ¬ì‚¬í•­", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}},
                    {{"ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸": "FUN-004", "ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜": "í˜¸í™˜ì„±í‘œì¤€", "ìš”êµ¬ì‚¬í•­_ëª…ì¹­": "í˜¸í™˜ì„± ìš”êµ¬ì‚¬í•­", "ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©": "ì‹¤ì œ ì„¸ë¶€ë‚´ìš©", "ì‚°ì¶œì •ë³´": ["ì‹¤ì œ ì‚°ì¶œë¬¼"]}}
                ],
                "ê¸°ëŠ¥ìš”êµ¬": "ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸ë³„ í•µì‹¬ ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­",
                "ì¸í„°í˜ì´ìŠ¤ì—°ê³„": "ì‹œìŠ¤í…œ ëª©ë¡, ì—°ê³„ ë°©ì‹, ì£¼ê¸°",
                "ë°ì´í„°": "ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸ë³„ ë°ì´í„° ê´€ë ¨ ìš”êµ¬ì‚¬í•­",
                "ë¹„ê¸°ëŠ¥ìš”êµ¬": "ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸ë³„ ì„±ëŠ¥, ê°€ìš©ì„±, í™•ì¥ì„±, ë³´ì•ˆ, ì ‘ê·¼ì„± ìš”êµ¬ì‚¬í•­",
                "í˜¸í™˜ì„±í‘œì¤€": "êµ­ê°€í‘œì¤€, ì˜¤í”ˆAPI, ë¸Œë¼ìš°ì €/OS í˜¸í™˜ì„±"
            }},
            "6_ë³´ì•ˆì¤€ë²•": {{
                "ì¸ì¦ê¶Œí•œê°ì‚¬": "ë¡œê·¸, ë¶„ë¦¬, ì¶”ì ì„±",
                "ê°œì¸ì •ë³´ì»´í”Œë¼ì´ì–¸ìŠ¤": "ISO27001, ISMS, GDPR ë“±",
                "ë§êµ¬ì„±ì•”í˜¸í™”": "ë§êµ¬ì„±, ì•”í˜¸í™”, í‚¤ê´€ë¦¬",
                "ì·¨ì•½ì ì§„ë‹¨": "ì·¨ì•½ì  ì§„ë‹¨ ë° ë³´ì•ˆì ê²€ ëŒ€ì‘"
            }},
            "7_ì„œë¹„ìŠ¤ìˆ˜ì¤€ìš´ì˜": {{
                "SLA": "ê°€ìš©ì„±, ì‘ë‹µ/ë³µêµ¬ ì‹œê°„, í˜ë„í‹°",
                "ì¥ì• ë³€ê²½ë°°í¬": "ITSM, CAB í”„ë¡œì„¸ìŠ¤",
                "ëª¨ë‹ˆí„°ë§ë¦¬í¬íŒ…": "KPI, ì£¼ê¸°, í¬ë§·",
                "í—¬í”„ë°ìŠ¤í¬": "ì§€ì› ì‹œê°„ ë° í‹°ì–´",
                "êµìœ¡ë§¤ë‰´ì–¼": "êµìœ¡, ë§¤ë‰´ì–¼, ì „í™˜ìš´ì˜, ì¼€ì–´ê¸°ê°„"
            }},
            "8_í’ˆì§ˆê²€ìˆ˜ì¸ìˆ˜": {{
                "ì‚°ì¶œë¬¼ëª©ë¡": "ì‚°ì¶œë¬¼ ëª©ë¡ ë° í…œí”Œë¦¿",
                "í…ŒìŠ¤íŠ¸ê³„íš": "ë‹¨ìœ„/í†µí•©/ì„±ëŠ¥/UAT í…ŒìŠ¤íŠ¸ ê³„íš",
                "ì¸ìˆ˜ê¸°ì¤€": "ì¸ìˆ˜ ê¸°ì¤€, ê²°í•¨ í—ˆìš©ì¹˜, ì¬ê²€ìˆ˜ ê·œì¹™",
                "íŒŒì¼ëŸ¿PoC": "íŒŒì¼ëŸ¿/PoC ì¡°ê±´"
            }},
            "9_ê³„ì•½ë²•ë¬´": {{
                "ê³„ì•½ìœ í˜•": "ì´ì•¡/ë‹¨ê°€/ì„±ê³¼í˜• ê³„ì•½",
                "ì§€ì ì¬ì‚°ê¶Œ": "ì†ŒìŠ¤ì½”ë“œ ì†Œìœ  ë° ì‚¬ìš©ê¶Œ",
                "ë¹„ë°€ìœ ì§€": "NDA, ìë£Œë°˜í™˜ ì¡°ê±´",
                "ì†í•´ë°°ìƒ": "ì†í•´ë°°ìƒ, ì§€ì²´ìƒê¸ˆ, ë³´ì¦, ë³´í—˜",
                "í•˜ìë³´ìˆ˜": "í•˜ìë³´ìˆ˜ ê¸°ê°„ ë° ë²”ìœ„"
            }},
            "10_ê³µê¸‰ì‚¬ìê²©ì—­ëŸ‰": {{
                "ì°¸ì—¬ì œí•œ": "ì—…ì¢…, ë“±ê¸‰, ì‹¤ì  ë“± ì°¸ì—¬ ì œí•œ",
                "í•„ìˆ˜ìê²©": "í•„ìˆ˜ ìê²© ìš”ê±´",
                "íˆ¬ì…ì¸ë ¥": "ë“±ê¸‰, ìê²©ì¦, ìƒì£¼ ì—¬ë¶€",
                "ë ˆí¼ëŸ°ìŠ¤": "ìœ ì‚¬ í”„ë¡œì íŠ¸ ê·œëª¨, ê¸°ê°„, ê¸°ìˆ ìŠ¤íƒ"
            }},
            "11_ì œì¶œí˜•ì‹ì§€ì‹œ": {{
                "ì œì•ˆì„œí˜•ì‹": "ì œì•ˆì„œ í˜•ì‹, ë¶„ëŸ‰, ì–¸ì–´, íŒŒì¼ ê·œê²©",
                "í•„ìˆ˜ì²¨ë¶€": "ì„œì•½ì„œ, ì¸ì¦ì„œ, ì¬ë¬´ì œí‘œ ë“±",
                "ì œì¶œì±„ë„": "ì œì¶œ ì±„ë„, ì›ë³¸/ì‚¬ë³¸ ë§¤ìˆ˜",
                "í”„ë ˆì  í…Œì´ì…˜": "ë°ëª¨/ìƒ˜í”Œ/ì‹œì—° ìš”êµ¬ ê¸°ì¤€"
            }},
            "ê¸°ìˆ ì†”ë£¨ì…˜ë§¤í•‘": {{"ìš”êµ¬ì‚¬í•­": "êµ¬ì²´ì ì¸ ê¸°ìˆ  ì†”ë£¨ì…˜ ëª…"}},
            "í•µì‹¬í‚¤ì›Œë“œ": ["í•µì‹¬ í‚¤ì›Œë“œë“¤"]
        }}
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ RFP ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ RFP ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                st.error("âŒ GPT ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {}
                
        except Exception as e:
            st.error(f"âŒ RFP ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
    
    def store_rfp_in_search(self, rfp_analysis: Dict[str, Any], rfp_content: str, pdf_title: str = None):
        """ë¶„ì„ëœ RFPë¥¼ Azure AI Searchì— ì €ì¥"""
        try:
            # ì„ë² ë”© ë²¡í„° ìƒì„±
            content_vector = self.get_embedding(rfp_content)
            
            # PDF ì œëª© ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: ì „ë‹¬ë°›ì€ pdf_title > í”„ë¡œì íŠ¸ ê°œìš” > ê¸°ë³¸ê°’)
            if pdf_title:
                document_title = pdf_title
            else:
                project_overview = ""
                if "1_í•µì‹¬ê°œìš”" in rfp_analysis:
                    core_overview = rfp_analysis["1_í•µì‹¬ê°œìš”"]
                    project_overview = f"{core_overview.get('ë°°ê²½ëª©ì ', '')} {core_overview.get('ê¸°ëŒ€ì„±ê³¼', '')}".strip()
                document_title = project_overview[:100] if project_overview else "RFP Document"
            
            # ì˜ˆì‚° ë²”ìœ„ ì¶”ì¶œ
            budget_range = ""
            if "3_ì˜ˆì‚°ê°€ê²©" in rfp_analysis:
                budget_info = rfp_analysis["3_ì˜ˆì‚°ê°€ê²©"]
                budget_range = budget_info.get("ì¶”ì •ì˜ˆì‚°", "")
            
            # ì œì¶œ ë§ˆê°ì¼ ì¶”ì¶œ
            submission_deadline = ""
            if "2_ì¼ì •ë§ˆì¼ìŠ¤í†¤" in rfp_analysis:
                schedule_info = rfp_analysis["2_ì¼ì •ë§ˆì¼ìŠ¤í†¤"]
                submission_deadline = schedule_info.get("ì§ˆì˜ì‘ë‹µë§ˆê°", "")
            
            # ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            technical_requirements = []
            if "5_ìš”êµ¬ì‚¬í•­" in rfp_analysis:
                requirements_info = rfp_analysis["5_ìš”êµ¬ì‚¬í•­"]
                technical_requirements = [
                    requirements_info.get("ê¸°ëŠ¥ìš”êµ¬", ""),
                    requirements_info.get("ë¹„ê¸°ëŠ¥ìš”êµ¬", ""),
                    requirements_info.get("ì¸í„°í˜ì´ìŠ¤ì—°ê³„", ""),
                    requirements_info.get("ë°ì´í„°", ""),
                    requirements_info.get("í˜¸í™˜ì„±í‘œì¤€", "")
                ]
                technical_requirements = [req for req in technical_requirements if req]
            
            # í‰ê°€ ê¸°ì¤€ ì¶”ì¶œ
            evaluation_criteria = {}
            if "4_í‰ê°€ì„ ì •ê¸°ì¤€" in rfp_analysis:
                eval_info = rfp_analysis["4_í‰ê°€ì„ ì •ê¸°ì¤€"]
                evaluation_criteria = {
                    "ì •ëŸ‰ì •ì„±ë°°ì ": eval_info.get("ì •ëŸ‰ì •ì„±ë°°ì ", ""),
                    "ê°€ì ê°ì ìš”ê±´": eval_info.get("ê°€ì ê°ì ìš”ê±´", ""),
                    "íƒˆë½í•„ìˆ˜ìš”ê±´": eval_info.get("íƒˆë½í•„ìˆ˜ìš”ê±´", "")
                }
            
            document = {
                "id": f"rfp_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "title": document_title,
                "content": rfp_content,
                "requirements": json.dumps(technical_requirements, ensure_ascii=False),
                "project_type": rfp_analysis.get("project_type", ""),
                "budget_range": budget_range,
                "submission_deadline": submission_deadline,
                "evaluation_criteria": json.dumps(evaluation_criteria, ensure_ascii=False),
                "created_date": datetime.now().isoformat() + "Z",
                "content_vector": content_vector
            }
            
            result = self.search_client.upload_documents([document])
            return len(result) > 0
            
        except Exception as e:
            st.error(f"âŒ RFP ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def search_similar_rfps(self, current_rfp_keywords: List[str], limit: int = 5):
        """ìœ ì‚¬í•œ RFP ê²€ìƒ‰"""
        try:
            # í‚¤ì›Œë“œë¥¼ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_query = " ".join(current_rfp_keywords)
            
            # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ìƒì„±
            query_vector = self.get_embedding(search_query)
            
            if not query_vector:
                return []
            
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            vector_query = VectorizedQuery(vector=query_vector, k_nearest_neighbors=limit, fields="content_vector")
            
            results = self.search_client.search(
                search_text=search_query,
                vector_queries=[vector_query],
                select=["title", "project_type", "requirements", "evaluation_criteria", "created_date"],
                top=limit
            )
            
            similar_rfps = []
            for result in results:
                similar_rfps.append({
                    "title": result["title"],
                    "project_type": result["project_type"],
                    "requirements": result["requirements"],
                    "evaluation_criteria": result["evaluation_criteria"],
                    "created_date": result["created_date"],
                    "score": result.get("@search.score", 0)
                })
            
            return similar_rfps
            
        except Exception as e:
            st.error(f"âŒ ìœ ì‚¬ RFP ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def ask_question_about_rfp(self, question: str, rfp_content: str, rfp_analysis: Dict[str, Any] = None) -> str:
        """RFP ë‚´ìš©ì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µ"""
        try:
            # RFP ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ êµ¬ì¡°í™”ëœ ì •ë³´ë„ í¬í•¨
            analysis_context = ""
            if rfp_analysis:
                analysis_context = f"""
                
                RFP ë¶„ì„ ê²°ê³¼:
                {json.dumps(rfp_analysis, ensure_ascii=False, indent=2)}
                """
            
            prompt = f"""
            RFP ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
            
            RFP ë‚´ìš©:
            {rfp_content[:3000]}  # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ 3000ìë¡œ ì œí•œ
            
            {analysis_context}
            
            ì‚¬ìš©ì ì§ˆë¬¸: {question}
            
            ë‹µë³€ ì§€ì¹¨:
            1. RFP ë¬¸ì„œì—ì„œ ì§ì ‘ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ìš°ì„  ì œê³µ
            2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì¡°ê±´, ì¼ì • ë“± ì •í™•íˆ ëª…ì‹œ
            3. ê´€ë ¨ ì¡°í•­ì´ë‚˜ ì„¹ì…˜ ì°¸ì¡°
            4. ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ "ë¬¸ì„œì—ì„œ ëª…í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" í‘œì‹œ
            5. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
            
            ë‹µë³€:
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ RFP ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. RFP ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            st.error(f"âŒ ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    # def generate_proposal_draft(self, rfp_analysis: Dict[str, Any], similar_rfps: List[Dict]) -> str:
    #     """ì œì•ˆì„œ ì´ˆì•ˆ ìƒì„±"""
    #     similar_rfps_text = ""
    #     for i, rfp in enumerate(similar_rfps[:3], 1):
    #         similar_rfps_text += f"\n{i}. {rfp['title']} (ìœ í˜•: {rfp['project_type']})\n"
    #     
    #     # RFP ë¶„ì„ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬
    #     rfp_summary = ""
    #     for category_key, category_data in rfp_analysis.items():
    #         if category_key.startswith(('1_', '2_', '3_', '4_', '5_', '6_', '7_', '8_', '9_', '10_', '11_')):
    #             category_name = category_key.replace('_', ' ').replace('1 ', '1. ').replace('2 ', '2. ').replace('3 ', '3. ').replace('4 ', '4. ').replace('5 ', '5. ').replace('6 ', '6. ').replace('7 ', '7. ').replace('8 ', '8. ').replace('9 ', '9. ').replace('10 ', '10. ').replace('11 ', '11. ')
    #             rfp_summary += f"\n{category_name}:\n"
    #             
    #             # ìš”êµ¬ì‚¬í•­ì˜ ê²½ìš° ìƒì„¸ëª©ë¡ì´ ìˆìœ¼ë©´ ê³ ìœ ë²ˆí˜¸ë³„ë¡œ í‘œì‹œ
    #             if category_key == "5_ìš”êµ¬ì‚¬í•­" and isinstance(category_data, dict) and "ìš”êµ¬ì‚¬í•­_ìƒì„¸ëª©ë¡" in category_data:
    #                 requirements_list = category_data.get("ìš”êµ¬ì‚¬í•­_ìƒì„¸ëª©ë¡", [])
    #                 if requirements_list:
    #                     rfp_summary += "  ìš”êµ¬ì‚¬í•­ ìƒì„¸ëª©ë¡:\n"
    #                     for req in requirements_list:
    #                         rfp_summary += f"    - {req.get('ìš”êµ¬ì‚¬í•­_ê³ ìœ ë²ˆí˜¸', 'N/A')}: {req.get('ìš”êµ¬ì‚¬í•­_ëª…ì¹­', 'N/A')}\n"
    #                         rfp_summary += f"      ë¶„ë¥˜: {req.get('ìš”êµ¬ì‚¬í•­_ë¶„ë¥˜', 'N/A')}\n"
    #                         rfp_summary += f"      ì„¸ë¶€ë‚´ìš©: {req.get('ìš”êµ¬ì‚¬í•­_ì„¸ë¶€ë‚´ìš©', 'N/A')}\n"
    #                         if req.get('ì‚°ì¶œì •ë³´'):
    #                             rfp_summary += f"      ì‚°ì¶œì •ë³´: {', '.join(req.get('ì‚°ì¶œì •ë³´', []))}\n"
    #                         rfp_summary += "\n"
    #             
    #             if isinstance(category_data, dict):
    #                 for key, value in category_data.items():
    #                     if value and key != "ìš”êµ¬ì‚¬í•­_ìƒì„¸ëª©ë¡":
    #                         rfp_summary += f"  - {key}: {value}\n"
    #             else:
    #                 rfp_summary += f"  {category_data}\n"
    #     
    #     prompt = f"""
    #     ë‹¤ìŒ RFP ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì œì•ˆì„œ ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

    #     RFP ë¶„ì„ ê²°ê³¼:
    #     {rfp_summary}

    #     ê¸°ìˆ  ì†”ë£¨ì…˜ ë§¤í•‘: {json.dumps(rfp_analysis.get('ê¸°ìˆ ì†”ë£¨ì…˜ë§¤í•‘', {}), ensure_ascii=False)}

    #     ìœ ì‚¬ í”„ë¡œì íŠ¸ ì‚¬ë¡€:{similar_rfps_text}

    #     **ì œì•ˆì„œ ì‘ì„± ì§€ì¹¨:**
    #     - ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸(ì˜ˆ: ECR-HWR-SVR-02, REQ-XXX-XXX ë“±)ë¥¼ ëª…ì‹œí•˜ì—¬ ê° ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì†”ë£¨ì…˜ì„ ì œì‹œí•˜ì„¸ìš”
    #     - ê° ìš”êµ¬ì‚¬í•­ë³„ë¡œ ì–´ë–»ê²Œ í•´ê²°í•  ê²ƒì¸ì§€ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
    #     - ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸ì™€ í•¨ê»˜ ì‚°ì¶œë¬¼ë„ ì–¸ê¸‰í•˜ì„¸ìš” (ì˜ˆ: ECR-HWR-SVR-02 ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ ì œì¶œ)

    #     ë‹¤ìŒ êµ¬ì¡°ë¡œ ì œì•ˆì„œ ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

    #     **â… . ì œì•ˆ ê°œìš”**
    #     - ì‚¬ì—…ì´í•´ë„

    #     **â…¡. ì œì•ˆ ì—…ì²´ ì¼ë°˜ (KT DS)**
    #     1. ì¼ë°˜ í˜„í™©
    #     2. ì¡°ì§ ë° ì¸ì›
    #     3. ì£¼ìš” ì‚¬ì—… ë‚´ìš©
    #     4. ì£¼ìš” ì‚¬ì—… ì‹¤ì 

    #     **â…¢. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ë¶€ë¬¸**
    #     1. ì¶”ì§„ ì „ëµ ë° ê°œë°œ ë°©ë²•ë¡ 
    #     2. ì‹œìŠ¤í…œ êµ¬ì„±ë„
    #     3. ì‹œìŠ¤í…œ êµ¬ì¶•
    #         > 3.1 ê°œë°œëŒ€ìƒì—…ë¬´ ë‚´ì—­ ë° êµ¬ì„±ìš”ê±´
    #         > 3.2 ì—°ê³„ ë²”ìœ„
    #         > 3.3 í‘œì¤€í™” ìš”ê±´
    #         > 3.4 ë³´ì•ˆ ë° ì›¹ í‘œì¤€í™”, ì„±ëŠ¥ì‹œí—˜ ìš”êµ¬ì‚¬í•­
    #         > 3.5 í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ë°©ì•ˆ

    #     **â…£. í”„ë¡œì íŠ¸ ê´€ë¦¬ ë¶€ë¬¸**
    #     1. í”„ë¡œì íŠ¸ ê´€ë¦¬ ë°©ë²•ë¡ 
    #     2. ì¶”ì§„ ì¼ì • ê³„íš
    #     3. íˆ¬ì… ì¸ë ¥ ë° ì´ë ¥ ì‚¬í•­

    #     **â…¤. ì§€ì› ë¶€ë¬¸**
    #     1. êµìœ¡ í›ˆë ¨ ê³„íš
    #     2. ê¸°ìˆ ì§€ì› ê³„íš
    #     3. í•˜ìë³´ìˆ˜ ê³„íš
    #     4. ì•ˆì •í™”
    #     """
    #     
    #     try:
    #         response = self.openai_client.chat.completions.create(
    #             model="gpt-4o-mini",
    #             messages=[
    #                 {"role": "system", "content": "ë‹¹ì‹ ì€ ê²½í—˜ì´ í’ë¶€í•œ ì œì•ˆì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì„¤ë“ë ¥ ìˆëŠ” ì œì•ˆì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."},
    #                 {"role": "user", "content": prompt}
    #             ],
    #             temperature=0.5,
    #             max_tokens=3000
    #         )
    #         
    #         return response.choices[0].message.content
    #         
    #     except Exception as e:
    #         st.error(f"âŒ ì œì•ˆì„œ ì´ˆì•ˆ ìƒì„± ì˜¤ë¥˜: {str(e)}")
    #         return ""
